# -*- coding: utf-8 -*-
# @Time    : 2023/9/4 ä¸‹åˆ11:24
# @Author  : sudoskys
# @File    : sublimate.py
# @Software: PyCharm
import re
from typing import List

import numpy as np
from pydantic import BaseModel

from .evaluate import Cut, Extraction, Sim

# å…­è¾¹å½¢è¯„æµ‹ï¼ŒæŒ‰ç…§æ’ååˆ†é…åˆå§‹åˆ†æ•°ã€‚
# é‡‡ç”¨åˆ‡åˆ†åˆ†è£‚æ¡ç›®ï¼Œè¿‡æ»¤æ— æ„ä¹‰è¯­å¥ã€‚
# é‡‡ç”¨å…³é”®è¯ç­›é€‰æ¸…æ´— ï¼ŒæŒ‘é€‰ã€‚
# ç›¸ä¼¼åº¦æ’åºï¼Œè£…ç®±ã€‚
BREAK_SHORT = [
    "è¨‚é–±", "è®¢é˜…", "æ’­æ”¾é‡", "ç‚¹èµ",
    "ç›¸å…³è§†é¢‘", "é‡è¯•", "ä¸¾æŠ¥", "å†…å®¹åˆä½œ", "ICPå¤‡", "å…¬ç½‘å®‰å¤‡", "å“”å“©å“”å“©bilibili_",
    "è‡ªåŠ¨è¿æ’­", "æŠ–éŸ³çŸ­è§†é¢‘", "çŸ­è§†é¢‘", "æ›´å¤šç²¾å½©", "åªçœ‹æ¥¼ä¸»", "ä¸Šä¸€ç¯‡", "#çƒ­è®®#",
    "å…³æ³¨é—®é¢˜", "ğŸ”", "ä¸‹è½½APP", "è¿æ³•æœ‰å®³ä¿¡æ¯"
]


class Order(BaseModel):
    text: str
    x_order: int = -1
    y_score: int = -1
    z_sim: int = -1


class Sublimate(object):

    def __init__(self, sentences: List[str]):
        self.origin = sentences
        self.x_factor = 9  # åˆå§‹é¡ºåº
        self.y_factor = 3  # ä¸»é¢˜å¾—åˆ†
        self.z_factor = 1  # ç›¸ä¼¼åº¦æ£€æŸ¥
        self.valuate: List[Order] = []
        if not self.origin:
            raise ValueError("Bad Arg For Sublimate")

    @staticmethod
    def _count_in(key_list: List[str], target: str):
        _count = 0
        for item in key_list:
            if item in target:
                _count = _count + 1
        return _count

    @staticmethod
    def real_len(string):
        _r = re.findall(r"[^Wd]+", string)
        return len("".join(_r))

    def wipe_sentence(self, min_limit: int, must_contain: List[str] = None):
        _wipe = self.valuate.copy()
        _new_copy = []
        for item_obj in _wipe:
            if self.real_len(item_obj.text) < min_limit:
                continue
            if self._count_in(BREAK_SHORT, item_obj.text) != 0:
                continue
            if must_contain:
                if self._count_in(must_contain, item_obj.text) != len(must_contain):
                    continue
            _new_copy.append(item_obj)
        #
        self.valuate: List[Order] = _new_copy

    def valuation(self, match_sentence: str, match_keywords: List[str] = None, min_limit: int = 13):
        # åˆ†å‰²å¹¶èµ‹äºˆä½ç½®
        self.valuate: List[Order] = []
        for index, item in enumerate(self.origin):
            _child = [item]
            if index > 3 and len(item) > 20:
                _child = Cut().cut_sentence(item)
            __score = ((1 - (index / len(self.origin))) * 100) * self.x_factor  # è¯„ä¼°è®¡ç®— order,ä¹˜å½±å“å› å­
            for child_item in _child:
                self.valuate.append(Order(text=child_item, x_order=__score))

        # æ¸…æ´—æ•°æ®
        self.wipe_sentence(min_limit=min_limit, must_contain=match_keywords)
        if not self.valuate:
            return []

        # æå–å¥å­æˆåˆ†
        _keywords = Extraction.tfidf_keywords(match_sentence)  # é«˜å¼€é”€æ³¨æ„
        # å¯¹å…¶ç­›é€‰è¯„åˆ†
        __total = len(_keywords)
        for order_obj in self.valuate:
            # ä¸ºæ¯ä¸ªæ¡ç›®èµ‹äºˆä¸»é¢˜è¯„åˆ†
            __score = 0
            for key in _keywords:
                if key in order_obj.text:
                    __score = __score + 1
            # è®¡ç®—å½“å‰æ¡ç›®åˆ†æ•°
            _y_score = ((__score / __total + 1) * 100) * self.y_factor
            order_obj.y_score = _y_score

        # ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•
        for order_obj in self.valuate:
            # é«˜å¼€é”€æ³¨æ„
            _z_sim = ((Sim.cosion_similarity(pre=match_sentence, aft=order_obj.text)) * 100) * self.z_factor
            order_obj.z_sim = _z_sim

        # è®¡ç®—
        # åˆå§‹åŒ–ç‚¹
        origin = np.array((0, 0, 0))
        _result = {}
        for item_obj in self.valuate:
            _target = np.array((item_obj.x_order, item_obj.y_score, item_obj.z_sim))
            _result[item_obj.text] = float(np.linalg.norm(origin - _target))
        return sorted(_result.items(), reverse=True, key=lambda x: x[1])
