# -*- coding: utf-8 -*-
# @Time    : 2023/9/25 ä¸‹åˆ10:48
# @Author  : sudoskys
# @File    : receiver_client.py
# @Software: PyCharm
#####
# This file is not a top-level schematic file!
#####

import os
import ssl
from abc import ABCMeta, abstractmethod
from typing import Optional

from aio_pika.abc import AbstractIncomingMessage
from loguru import logger

from llmkira.middleware.chain_box import Chain, ChainReloader
from llmkira.middleware.llm_task import OpenaiMiddleware
from llmkira.schema import RawMessage
from llmkira.sdk.error import RateLimitError
from llmkira.sdk.func_calling import ToolRegister
from llmkira.task import Task, TaskHeader


class BaseSender(object, metaclass=ABCMeta):
    @abstractmethod
    async def file_forward(self, receiver, file_list, **kwargs):
        pass

    @abstractmethod
    async def forward(self, receiver, message, **kwargs):
        """
        æ’ä»¶ä¸“ç”¨è½¬å‘ï¼Œæ˜¯Taské€šç”¨ç±»å‹
        """
        pass

    @abstractmethod
    async def reply(self, receiver, message, **kwargs):
        """
        æ¨¡å‹ç›´è½¬å‘ï¼ŒMessageæ˜¯Openaiçš„ç±»å‹
        """
        pass

    @abstractmethod
    async def error(self, receiver, text, **kwargs):
        pass

    @abstractmethod
    async def function(self, receiver, task, llm, result, message, **kwargs):
        pass


class BaseReceiver(object):
    def __init__(self):
        self.sender: Optional[BaseSender] = None
        self.task: Optional[Task] = None

    def set_core(self, sender: BaseSender, task: Task):
        self.sender = sender
        self.task = task

    @staticmethod
    async def llm_request(llm_agent: OpenaiMiddleware, disable_function: bool = False):
        """
        æ ¡éªŒåŒ…è£…ï¼Œæ²¡æœ‰å…¶ä»–ä½œç”¨
        """
        try:
            _result = await llm_agent.request_openai(disable_function=disable_function)
            _message = _result.default_message
            logger.debug(f"[x] LLM Message Sent \n--message {_message}")
            assert _message, "message is empty"
            return _result
        except ssl.SSLSyscallError as e:
            logger.error(f"Network ssl error: {e},that maybe caused by bad proxy")
            raise e
        except RateLimitError as e:
            logger.error(f"ApiEndPoint:{e}")
            raise ValueError(f"Authentication expiration, overload or other issues with the Api Endpoint")
        except Exception as e:
            logger.exception(e)
            raise e

    async def _flash(self,
                     task: TaskHeader,
                     llm: OpenaiMiddleware,
                     auto_write_back: bool = True,
                     intercept_function: bool = False,
                     disable_function: bool = False
                     ):
        """
        å‡½æ•°æ± åˆ·æ–°
        :param auto_write_back: æ˜¯å¦å°†taskæºå¸¦çš„æ¶ˆæ¯å›å†™è¿›æ¶ˆæ¯æ± ä¸­ï¼Œå¦‚æœä¸ºFalseåˆ™ä¸¢å¼ƒtaskæºå¸¦æ¶ˆæ¯
        :param intercept_function: æ˜¯å¦æ‹¦æˆªå‡½æ•°è°ƒç”¨è½¬å‘åˆ°å‡½æ•°å¤„ç†å™¨
        """
        try:
            llm.build(auto_write_back=auto_write_back)
            try:
                result = await self.llm_request(llm, disable_function=disable_function)
            except Exception as e:
                await self.sender.error(
                    receiver=task.receiver,
                    text=f"ğŸ¦´ Sorry, your request failed because: {e}"
                )
                return
            if intercept_function:
                # æ‹¦æˆªå‡½æ•°è°ƒç”¨
                if hasattr(result.default_message, "function_call"):
                    return await self.sender.function(
                        receiver=task.receiver,
                        task=task,
                        llm=llm,  # IMPORTANT
                        message=result.default_message,
                        result=result
                    )
            return await self.sender.reply(
                receiver=task.receiver,
                message=[result.default_message]
            )
        except Exception as e:
            raise e

    async def deal_message(self, message):
        """
        å¤„ç†æ¶ˆæ¯
        """
        # è§£ææ•°æ®
        _task: TaskHeader = TaskHeader.parse_raw(message.body)
        # æ²¡æœ‰ä»»ä½•å‚æ•°
        if _task.task_meta.direct_reply:
            await self.sender.forward(
                receiver=_task.receiver,
                message=_task.message
            )
            return None, None, None
        # å‡½æ•°é‡æ•´ç­–ç•¥
        functions = []
        if _task.task_meta.function_enable:
            # ç»§æ‰¿å‡½æ•°
            functions = _task.task_meta.function_list
            if _task.task_meta.sign_as[0] == 0:
                # å¤åˆ¶æ•‘èµ
                _task.task_meta.function_salvation_list = _task.task_meta.function_list
                functions = []
                # é‡æ•´
                for _index, _message in enumerate(_task.message):
                    _message: RawMessage
                    functions.extend(
                        ToolRegister().filter_pair(key_phrases=_message.text, file_list=_message.file)
                    )
                _task.task_meta.function_list = functions
        if _task.task_meta.sign_as[0] == 0:
            # å®¹é”™ä¸€å±‚æ—§èŠ‚ç‚¹
            functions.extend(_task.task_meta.function_salvation_list)
        # æ„å»ºé€šä¿¡ä»£ç†
        _llm = OpenaiMiddleware(task=_task, function=functions)
        logger.debug(f"[x] Received Order \n--order {_task.json(indent=2)}")
        # æ’ä»¶ç›´æ¥è½¬å‘ä¸é‡å¤„ç†
        if _task.task_meta.callback_forward:
            # æ‰‹åŠ¨è¿½åŠ æ’ä»¶äº§ç”Ÿçš„çº¿ç´¢æ¶ˆæ¯
            _llm.write_back(
                role=_task.task_meta.callback.role,
                name=_task.task_meta.callback.name,
                message_list=_task.message
            )
            # æ’ä»¶æ•°æ®å“åº”åˆ°å‰ç«¯
            if _task.task_meta.callback_forward_reprocess:
                # æ‰‹åŠ¨å†™å›åˆ™ç¦ç”¨ä» Task æ•°æ®ä½“è‡ªåŠ¨å›å†™
                # é˜²æ­¢AIå»å¯åŠ¨å…¶ä»–å‡½æ•°ï¼Œç¦ç”¨å‡½æ•°
                await self._flash(
                    llm=_llm,
                    task=_task,
                    intercept_function=True,
                    disable_function=True,
                    auto_write_back=False
                )
                # åŒæ—¶é€’äº¤éƒ¨ç½²ç‚¹
                return _task, _llm, "callback_forward_reprocess"
            # è½¬å‘å‡½æ•°
            await self.sender.forward(
                receiver=_task.receiver,
                message=_task.message
            )
            # åŒæ—¶é€’äº¤éƒ¨ç½²ç‚¹
            return _task, _llm, "callback_forward_reprocess"
        await self._flash(llm=_llm, task=_task, intercept_function=True)
        return None, None, None

    async def on_message(self, message: AbstractIncomingMessage):
        if not self.task or not self.sender:
            raise ValueError("receiver not set core")
        try:
            if os.getenv("LLMBOT_STOP_REPLY") == "1":
                return None
            _task, _llm, _point = await self.deal_message(message)
            # å¯åŠ¨é“¾å¼å‡½æ•°åº”ç­”å¾ªç¯
            if _task:
                chain: Chain = await ChainReloader(uid=_task.receiver.uid).get_task()
                if chain:
                    logger.info(f"Catch chain callback\n--callback_send_by {_point}")
                    await Task(queue=chain.address).send_task(task=chain.arg)
        except Exception as e:
            logger.exception(e)
            await message.reject(requeue=False)
        finally:
            await message.ack(multiple=False)
